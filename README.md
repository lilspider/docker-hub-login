# Docker Hub Login with GitHub Actions

This repository demonstrates how to log into Docker Hub using GitHub Actions and secrets.

## ğŸ”§ Setup Required

### 1. Create Docker Hub Access Token
1. Go to [Docker Hub](https://hub.docker.com/)
2. Click your profile â†’ **Account Settings**
3. Go to **Security** tab
4. Click **New Access Token**
5. Give it a name (e.g., "github-actions")
6. Copy the generated token

## ğŸ“ Project Structure

```
â”œâ”€â”€ data/
â”‚   â””â”€â”€ input.csv          # Source data file
â”œâ”€â”€ output/                # Generated output files
â”‚   â”œâ”€â”€ transformed_data.csv
â”‚   â””â”€â”€ transformed_data.json
â”œâ”€â”€ etl.py                 # Main ETL script
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ .github/workflows/
    â””â”€â”€ etl-pipeline.yml   # GitHub Actions workflow
```

## ğŸ”„ ETL Process

### Extract
- Reads employee data from `data/input.csv`
- Loads 10 sample records with employee information

### Transform
- Adds `salary_category` (High/Medium/Low based on salary)
- Adds `age_group` (Senior/Mid/Junior based on age)
- Formats names to title case
- Adds processing timestamp

### Load
- Saves transformed data to `output/transformed_data.csv`
- Saves structured data with metadata to `output/transformed_data.json`

## ğŸ¤– GitHub Actions

The workflow triggers on:
- **Push** to master branch
- **Pull requests** to master branch
- **Manual dispatch** (run manually)
- **Daily schedule** at 2:00 AM UTC

**Features:**
- Runs on Ubuntu latest
- Uses Python 3.11
- Installs dependencies automatically
- Uploads output artifacts
- Displays execution summary

## ğŸ› ï¸ Local Development

1. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

2. Run the ETL pipeline:
   ```bash
   python etl.py
   ```

3. Check the output in the `output/` directory

## ğŸ“Š Sample Output

The pipeline generates:
- **CSV**: Clean, transformed data ready for analysis
- **JSON**: Structured data with metadata including:
  - Total record count
  - Processing timestamp
  - Column information
  - All transformed records

## ğŸ”§ Next Steps

Extend this ETL pipeline by:
- Adding data validation steps
- Connecting to real data sources (databases, APIs)
- Implementing error handling and logging
- Adding data quality checks
- Connecting to cloud storage (S3, GCS)
- Adding notification systems
