# Docker Hub Login with GitHub Actions

This repository demonstrates how to log into Docker Hub using GitHub Actions and secrets.

## ğŸ”§ Setup Required

### 1. Create Docker Hub Access Token
1. Go to [Docker Hub](https://hub.docker.com/)
2. Click your profile â†’ **Account Settings**
3. Go to **Security** tab
4. Click **New Access Token**
5. Give it a name (e.g., "github-actions")
6. Copy the generated token

## ğŸ“ Project Structure

```
â”œâ”€â”€ data/
â”‚   â””â”€â”€ input.csv          # Source data file
â”œâ”€â”€ output/                # Generated output files
â”‚   â”œâ”€â”€ transformed_data.csv
â”‚   â””â”€â”€ transformed_data.json
â”œâ”€â”€ etl.py                 # Main ETL script
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ .github/workflows/
    â””â”€â”€ etl-pipeline.yml   # GitHub Actions workflow
```

## ğŸ”„ ETL Process

### Extract
- Reads employee data from `data/input.csv`
- Loads 10 sample records with employee information

### Transform
- Adds `salary_category` (High/Medium/Low based on salary)
- Adds `age_group` (Senior/Mid/Junior based on age)
- Formats names to title case
- Adds processing timestamp

### Load
- Saves transformed data to `output/transformed_data.csv`
- Saves structured data with metadata to `output/transformed_data.json`

## ğŸ¤– GitHub Actions

The workflow triggers on:
- **Push** to master branch
- **Pull requests** to master branch
- **Manual dispatch** (run manually)
- **Daily schedule** at 2:00 AM UTC

**Features:**
- Runs on Ubuntu latest
- Uses Python 3.11
- Installs dependencies automatically
- Uploads output artifacts
- Displays execution summary

## ğŸ› ï¸ Local Development

1. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

2. Run the ETL pipeline:
   ```bash
   python etl.py
   ```

3. Run tests:
   ```bash
   pytest
   # or for verbose output
   pytest -v
   ```

4. Check the output in the `output/` directory

## ğŸ§ª Testing

The project includes comprehensive pytest tests covering:

- **Unit Tests**: Individual function testing
  - `test_extract()` - Data extraction validation
  - `test_transform()` - Data transformation logic
  - `test_load_csv()` - CSV output verification
  - `test_load_json()` - JSON output verification

- **Integration Tests**: End-to-end pipeline testing
  - `test_full_pipeline_integration()` - Complete ETL flow
  - `test_data_integrity()` - Data preservation through pipeline

- **Edge Case Tests**: Boundary conditions
  - `test_edge_cases()` - Empty DataFrame handling
  - `test_salary_categorization()` - Salary boundary testing
  - `test_age_grouping()` - Age boundary testing

**Test Coverage:**
- âœ… Data extraction from CSV
- âœ… Data transformation logic
- âœ… Salary categorization (High/Medium/Low)
- âœ… Age grouping (Senior/Mid/Junior)
- âœ… Name formatting (title case)
- âœ… Output file generation (CSV & JSON)
- âœ… Data integrity preservation
- âœ… Error handling and edge cases

## ğŸ“Š Sample Output

The pipeline generates:
- **CSV**: Clean, transformed data ready for analysis
- **JSON**: Structured data with metadata including:
  - Total record count
  - Processing timestamp
  - Column information
  - All transformed records

## ğŸ”§ Next Steps

Extend this ETL pipeline by:
- Adding data validation steps
- Connecting to real data sources (databases, APIs)
- Implementing error handling and logging
- Adding data quality checks
- Connecting to cloud storage (S3, GCS)
- Adding notification systems
